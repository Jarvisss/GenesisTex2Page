<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <meta property="og:title" content="GenesisTex2: Stable, Consistent and High-Quality Text-to-Texture Generation"/>
  <meta property="og:url" content="https://https://jarvisss.github.io/GenesisTex2Page/"/>
  <meta property="og:image" content="static/figures/teaser.jpg" />
  <meta property="og:image:width" content="2603"/>
  <meta property="og:image:height" content="680"/>


  <title>GenesisTex2: Stable, Consistent and High-Quality Text-to-Texture Generation</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">
  <link rel="icon" href="static/figures/icon.png">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


<section class="publication-header">
  <div class="hero-body">
    <div class="container is-max-widescreen">
      <!-- <div class="columns is-centered"> -->
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">GenesisTex2: Stable, Consistent and High-Quality Text-to-Texture Generation</h1>
        </div>
    </div>
  </div>   
</section>

<section class="publication-author-block">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
		  <div class="is-size-4 publication-authors">
            <span class="author-block">Jiawei Lu<sup>1,2#</sup>,</span>
            <span class="author-block">Yingpeng Zhang<sup>2#*</sup>,</span>
            <span class="author-block">Zengjun Zhao<sup>2</sup>,</span>
            <span class="author-block">He Wang<sup>3</sup>,</span>
            <span class="author-block">Kun Zhou<sup>1</sup>,</span>
            <span class="author-block">Tianjia Shao<sup>1*</sup></span>
          </div>
          <div class="is-size-4 publication-authors">
            <span class="author-block"><sup>#</sup>Equal contribution</span><br>
            <span class="author-block"><sup>*</sup>Corresponding author</span>
          </div><br><br>
          <div class="is-size-4 publication-authors">
            <span class="author-block"><sup>1</sup>State Key Lab of CAD&CG, Zhejiang University</span><br>
            <span class="author-block"><sup>2</sup>Tencent IEG</span><br>
            <span class="author-block"><sup>3</sup>AI Centre, Computer Science, University College London</span>
          </div><br>
          <div class="is-size-5 publication-authors">
            <span class="author-block">AAAI 2025</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              
              <span class="link-block">
                <a href="./static/paper.pdf" target="_blank"
                  class="external-link button is-normal is-rounded">
                  <span class="icon">
                    <i class="fa fa-infinity"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="./static/supp.pdf" target="_blank"
                  class="external-link button is-normal is-rounded">
                  <span class="icon">
                    <i class="fa fa-images"></i>
                  </span>
                  <span>Supplementry</span>
                </a>
              </span>
              <span class="link-block">
                <a href="mailto:lujiawei23@gmail.com" target="_blank"
                  class="external-link button is-normal is-rounded">
                  <span class="icon">
                    <i class="fa fa-mail-bulk"></i>
                  </span>
                  <span>Contact</span>
                </a>
              </span>
            </div>
          </div>

        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="static/figures/examples.png" alt="Texturing results" width="800px"/>
      <h2 class="subtitle has-text-centered">
        Texturing results with GenesisTex2
      </h2>
    </div>
  </div>
</section>

<section class="section hero is-light">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <!-- div class="item">
          <p style="margin-bottom: 30px">
        <video poster="" id="tree" autoplay controls muted loop height="100%">
          <source src="static/figures/video.mp4"
          type="video/mp4">
        </video>
        </p>
        </div -->
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Large-scale text-guided image diffusion models have demonstrated remarkable results in text-to-image (T2I) generation. However, applying these models to synthesize textures for 3D geometries remains challenging due to the domain gap between 2D images and textures on a 3D surface. Early works that used a projecting-inpainting approach managed to preserve generation diversity, but often resulted in noticeable artifacts and style inconsistencies. While recent methods have attempted to address these inconsistencies, they often introduce other issues, such as blurring, over-saturation, or over-smoothing. To overcome these challenges, we propose a novel text-to-texture synthesis framework that takes advantage of pre-trained diffusion models. We introduce a local attention reweighing mechanism in the self-attention layers to guide the model in focusing on spatial-correlated patches across different views, thereby enhancing local details while preserving cross-view consistency. Additionally, we propose a novel latent space merge pipeline, which further ensures consistency across different viewpoints without sacrificing too much diversity. Our method significantly outperforms existing state-of-the-art techniques in terms of texture consistency and visual quality, while delivering results much faster than distillation-based methods. Importantly, our framework does not require additional training or fine-tuning, making it highly adaptable to a wide range of models available on public platforms.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>




<section class="section hero">
  <div class="container is-max-desktop">
    <h2 class="title has-text-centered">Video</h2>
    <div class="content">
      <video id="dollyzoom" autoplay controls muted loop playsinline height="100%">
        <source src="./static/videos/video_final.mp4"
                type="video/mp4">
      </video>
    </div>
  </div>
</section>


<section class="section hero is-small is-light">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">How does it work?</h2>
      </div>
    </div>

        <div class="content">
          <div class="column is-centered has-text-centered">
            <img src="static/figures/method.png" alt="Texturing Pipeline" width="100%"/>
          </div>
        </div>
      <h4 class="subtitle has-text-justified">
        Given a mesh and a textual prompt, we aim to produce textures that well depict the prompt and suit the shape. To achieve this, we propose a local attention technique which enhances local details by reweighing the original self-attention layers based on the 3D shape. In addition, we introduce a framework for consistent texture synthesis, enabling the stable generation of consistent and high-quality textures.
        </h4>
  
  </div>
</section> 

<section>
  <div class="container is-max-desktop">
    
        <h4 class="subtitle has-text-justified">
          Check out the <a href="./static/paper.pdf">paper</a> to learn more ðŸ¤“
        </h4>
  
  </div>
</section> 


<footer class="footer">
  <div class="columns is-centered">
    <div class="column is-8">
      <div class="content">
        <p>
          This website is licensed under a <a rel="license"
          href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
        Commons Attribution-ShareAlike 4.0 International License</a>.
      </p>
      <p>
        Credits for shapes are given in the paper. Please follow their respective licenses upon reuse.
    </p>
      <p>
        Website source code based on the <a href="https://nerfies.github.io/"> Nerfies</a> project page. If you want to reuse their <a
        href="https://github.com/nerfies/nerfies.github.io">source code</a>, please credit them appropriately.
      </p>
    </div>
  </div>
</div>
</div>
</footer>


<script type="text/javascript">
  var sc_project=12845175; 
  var sc_invisible=1; 
  var sc_security="c50db5bf"; 
  </script>
  <script type="text/javascript"
  src="https://www.statcounter.com/counter/counter.js" async></script>
  <noscript><div class="statcounter"><a title="Web Analytics"
  href="https://statcounter.com/" target="_blank"><img class="statcounter"
  src="https://c.statcounter.com/12845175/0/c50db5bf/1/" alt="Web Analytics"
  referrerPolicy="no-referrer-when-downgrade"></a></div></noscript>
  <!-- End of Statcounter Code -->

  </body>
  </html>
